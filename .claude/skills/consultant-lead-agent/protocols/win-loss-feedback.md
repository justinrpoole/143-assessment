---
name: win-loss-feedback-protocol
parent: consultant-lead-agent
version: 1.0
purpose: "Outcome-based learning that improves every component of the system"
---

# Win/Loss Feedback Protocol

## 1. Purpose

Most consulting practices repeat the same mistakes because they never systematically analyze why they win or lose. A principal will lose a deal, feel frustrated for a day, and then move on to the next opportunity without documenting what happened, what went wrong, or what the loss reveals about the system.

This protocol creates a feedback loop that compounds. Every deal — won or lost — teaches the system something. Won deals reveal what works so the system can replicate it. Lost deals reveal what fails so the system can correct it. Over time, the system gets smarter, faster, and more efficient at converting pipeline into revenue.

The feedback does not stay in a file. It routes to every agent in the system — research, outreach, competitive intelligence, and revenue — so that each agent can adjust its behavior based on real outcomes. The research agent learns which trigger types produce wins. The outreach agent learns which channels and angles work. The competitive agent learns why competitors beat us and how to counter them. The revenue agent recalibrates forecasting and pricing models.

This is the learning engine. Without it, the system is static. With it, the system is adaptive.

---

## 2. Win Analysis Protocol

### Trigger: SOW Signed

Within 48 hours of a signed SOW, the principal or business development lead completes the Win Analysis questionnaire. This is not optional. Every win gets analyzed.

### Win Analysis Questionnaire

```
WIN ANALYSIS — [Company Name] — [Date SOW Signed]

DEAL BASICS
  Company:                          [Name]
  Package(s) Sold:                  [A/B/C/D/E/F]
  Deal Value:                       $[X]
  Deal Cycle (days):                [N days from first outreach to SOW]
  Competitive Situation:            [Sole source / Competitive / Incumbent displacement]

ORIGINATION
  1. What was the trigger event that initiated this pursuit?
     [Specific event — e.g., "FERC filing revealed $200M grid mod program"]

  2. How many days between trigger detection and first outreach?
     [N days]

  3. Which outreach channel made the first real connection?
     [Email / LinkedIn / Warm introduction / Conference / Cold call / Direct mail]

  4. If warm introduction, who connected you?
     [Name, company, relationship context]

  5. Which marketing touches did the buyer recall seeing before the first conversation?
     [LinkedIn posts / Email campaigns / Conference presentations / Webinars / None / Unknown]

BUYER DYNAMICS
  6. Who was the primary buyer (name and title)?
     [Name, Title]

  7. Who was the internal champion (name and title)?
     [Name, Title]

  8. How did we identify the internal champion?
     [Referral / LinkedIn research / Conference meeting / Cold outreach response]

  9. What was the buyer's stated decision criteria (top 3)?
     a. [Criterion 1]
     b. [Criterion 2]
     c. [Criterion 3]

  10. What differentiated us from competitors (or from doing nothing)?
      [Specific differentiator — domain expertise, speed, diagnostic approach, team quality, price, relationship]

SALES PROCESS
  11. What was the biggest concern the buyer expressed during the sales process?
      [Concern and how it was addressed]

  12. What almost derailed the deal?
      [Specific risk moment — procurement delay, budget freeze, competitor counter-offer, internal politics]

  13. Which positioning angle resonated most?
      [Problem-agitate-solve / Insight lead / Case study / Provocative question / Peer reference / Regulatory urgency]

PRICING
  14. What price did the buyer expect to pay?
      $[X] (if known from conversation)

  15. What did we quote?
      $[X]

  16. Was there negotiation? What was the final price?
      $[X]

  17. Would the buyer have paid more? (Your honest assessment)
      [Y/N — if Y, how much more?]

POST-WIN
  18. What is the referral potential of this client? (1-5)
      [Score and reasoning]

  19. What is the case study potential? (1-5)
      [Score and reasoning]

  20. What would you replicate from this deal in future pursuits?
      [Specific tactical lessons]
```

### Win Analysis Routing

After each win analysis is completed, the findings are routed to the appropriate agents:

**To Research Agent:**
- Trigger type that produced this win (add weight to trigger scoring model)
- Intelligence source that detected the trigger (prioritize that source)
- Speed data: how many days from trigger to outreach? If <3 days and deal won, this reinforces speed-to-outreach priority

**To Outreach Agent:**
- Channel that made the first connection (increase allocation to that channel for similar targets)
- Positioning angle that resonated (use this angle more frequently in sequences for similar buyer personas)
- Marketing touches the buyer recalled (validate which content is actually being consumed)
- Internal champion identification method (replicate for future pursuits)

**To Competitive Agent:**
- Competitive situation details (who competed, why we won)
- Differentiators cited by buyer (document for competitive positioning playbook)
- Competitor weaknesses exposed (add to counter-positioning library)

**To Revenue Agent:**
- Full deal data for pipeline math recalibration
- Pricing signals (expected price vs quoted price vs final price)
- Deal cycle data (for stage velocity benchmarking)
- CLV baseline (entry engagement value as starting point for lifetime tracking)

**To Brand Memory (learnings.md):**
- Pattern documentation: "When [trigger type] occurs at [company type], using [channel] with [angle] and a [N]-day speed-to-outreach, the close rate is [X]%."
- This builds the institutional knowledge base that makes the system smarter over time.

---

## 3. Loss Analysis Protocol

### Trigger: Deal Lost at Any Stage

When a deal exits the pipeline for any reason other than a signed SOW, a Loss Analysis is triggered. The analysis must be completed within 7 business days of the loss.

Losses at later stages (Propose, Close) receive more detailed analysis than early-stage losses (Discover, Qualify) because they represent more invested effort and more information to learn from.

### Loss Analysis Questionnaire

```
LOSS ANALYSIS — [Company Name] — [Date Lost] — Stage: [Discover/Qualify/Engage/Propose/Close]

DEAL BASICS
  Company:                          [Name]
  Package(s) Pursued:               [A/B/C/D/E/F]
  Estimated Deal Value:             $[X]
  Time in Pipeline:                 [N days]
  Stage at Loss:                    [Discover / Qualify / Engage / Propose / Close]

LOSS CLASSIFICATION
  1. What was the stated reason for the loss?
     [Verbatim from buyer if possible, or best assessment]

  2. Loss type classification:
     [ ] Type A — Competitive (lost to another firm)
     [ ] Type B — No-decision (chose to do nothing)
     [ ] Type C — Budget (wanted it but could not fund it)
     [ ] Type D — Timing (wrong time, need may re-emerge)
     [ ] Type E — Qualification (was never a real opportunity)
     [ ] Type F — Relationship (preferred someone they already knew)

COMPETITIVE LOSS (Type A) — Complete if applicable:
  3. Which firm won?
     [Name]
  4. Why did they win? (buyer's stated reason)
     [Reason]
  5. How did their pricing compare to ours?
     [Higher / Lower / Similar / Unknown]
  6. What capability or positioning advantage did they have?
     [Specific advantage]
  7. Could we have won if we had done something differently?
     [Y/N — if Y, what specifically?]

NO-DECISION LOSS (Type B) — Complete if applicable:
  8. Why did the buyer choose inaction?
     [Budget reallocation / Priority shift / Internal politics / Fear of change / Unknown]
  9. Did we adequately convey the cost of inaction?
     [Y/N — what was missing?]
  10. Is there a future re-engagement trigger?
      [Y/N — if Y, what event would reopen this?]

BUDGET LOSS (Type C) — Complete if applicable:
  11. What was the buyer's budget constraint?
      $[X] or [description]
  12. What price would have won the deal?
      $[X] (if known)
  13. Could we have offered a smaller scope or phased approach?
      [Y/N — if Y, what would that look like?]

TIMING LOSS (Type D) — Complete if applicable:
  14. Why was the timing wrong?
      [Budget cycle / Reorganization / Leadership change / Other priority / Seasonal]
  15. When might the need re-emerge?
      [Q/Year estimate]
  16. Should this lead stay in the pipeline as a nurture?
      [Y/N]

QUALIFICATION LOSS (Type E) — Complete if applicable:
  17. Why was this not a real opportunity?
      [No budget authority / No real need / Information gathering only / Wrong ICP fit]
  18. What should the qualification process have caught earlier?
      [Specific screening question or criteria]
  19. How many hours/resources were invested before disqualification?
      [Estimate]

RELATIONSHIP LOSS (Type F) — Complete if applicable:
  20. Who did the buyer have a prior relationship with?
      [Name, firm, context]
  21. How could we have built a relationship earlier?
      [Conference / referral / LinkedIn engagement / content sharing]
  22. Is there a strategy to establish a relationship for future opportunities?
      [Y/N — if Y, specific approach]

ROOT CAUSE AND LEARNING
  23. Did we misread the trigger? Was the pain real or perceived?
      [Assessment]

  24. Did we reach the right buyer, or were we selling below the line?
      [Assessment]

  25. What would we do differently if we could replay this deal from the beginning?
      [Specific tactical changes]

  26. Is there a future re-engagement opportunity?
      [Y/N — if Y, set trigger date and re-engagement approach]
```

### Loss Classification Routing

Each loss type generates specific system corrections:

**Type A — Competitive Loss → Counter-positioning needed**
Route to: Competitive Agent
Action: Document the competitor's winning strategy. Add to counter-positioning playbook. Identify the specific capability or positioning gap. Develop response for future encounters with this competitor.
Research Agent: If the competitor is winning deals from a specific trigger type, increase monitoring to detect those triggers earlier (speed advantage).

**Type B — No-Decision Loss → Urgency messaging needed**
Route to: Outreach Agent
Action: Review the urgency messaging in the outreach sequence. Was the cost of inaction clearly communicated? Was the buyer's pain quantified in dollars? Develop stronger urgency frameworks. Add specific "cost of delay" calculations to outreach templates.
Revenue Agent: Flag no-decision losses as a segment. If >30% of losses are Type B, the entire system is failing to create urgency.

**Type C — Budget Loss → ROI argument or smaller entry point needed**
Route to: Revenue Agent + Outreach Agent
Action: Review whether a phased proposal or smaller diagnostic could have won. Develop a "budget-constrained" offer pathway: smaller scope diagnostic ($25K-$50K) that fits within discretionary spending limits. Update ROI templates with stronger financial justification.

**Type D — Timing Loss → Set re-engagement trigger**
Route to: Research Agent
Action: Set a future monitoring trigger for this company. Example: "Re-engage [Company] when their Q3 budget cycle opens in July." Add to nurture list with quarterly touchpoints. Do not disqualify — treat as a deferred opportunity.

**Type E — Qualification Loss → Scoring model adjustment needed**
Route to: Research Agent
Action: Review the qualification criteria. What signal did this lead have that made it look real? Add a disqualifying criterion to prevent similar false positives. Track hours wasted on Type E losses — this is the efficiency cost of poor qualification.

**Type F — Relationship Loss → Warm intro strategy needed**
Route to: Outreach Agent + Research Agent
Action: Identify who in the network has relationships at companies where we lack them. Invest in relationship-building at industry events, associations, and peer groups. Develop a 12-month relationship-building plan for the top 25 target accounts.

---

## 4. Stage Drop-Off Analysis

### Monthly Funnel Analysis

Every month, calculate the actual conversion rate at each pipeline stage and compare to targets.

```
STAGE DROP-OFF ANALYSIS — [Month Year]

                          Entered    Advanced    Dropped    Conv Rate    Target    Delta
  ──────────────────────────────────────────────────────────────────────────────────────
  Discover → Qualify        [N]        [N]         [N]       [X]%        40%      [+/-N]pp
  Qualify → Engage          [N]        [N]         [N]       [X]%        25%      [+/-N]pp
  Engage → Propose          [N]        [N]         [N]       [X]%        50%      [+/-N]pp
  Propose → Close           [N]        [N]         [N]       [X]%        30%      [+/-N]pp
  ──────────────────────────────────────────────────────────────────────────────────────
  Overall (Discover→Close)  [N]        [N]         [N]       [X]%        1.5%     [+/-N]pp
```

### Bottleneck Identification

The stage with the largest negative delta (actual vs target) is the bottleneck. Focus intervention on the bottleneck — improving a non-bottleneck stage has minimal impact on overall throughput.

```
BOTTLENECK ANALYSIS — [Month Year]

  Worst-Performing Stage:    [Stage Name]
  Conversion Rate:           [X]% (target: [Y]%)
  Delta:                     [Z] percentage points below target
  Leads Lost at This Stage:  [N] leads ($[X] potential revenue)

  Root Cause Analysis:
  ├── Hypothesis 1: [Description — e.g., "Outreach sequences are not reaching decision-makers"]
  │   Evidence: [Data supporting this hypothesis]
  │   Probability: [High / Medium / Low]
  │
  ├── Hypothesis 2: [Description — e.g., "Trigger events are not creating real urgency"]
  │   Evidence: [Data supporting this hypothesis]
  │   Probability: [High / Medium / Low]
  │
  └── Hypothesis 3: [Description — e.g., "Competitive landscape has shifted — new entrant offering lower prices"]
      Evidence: [Data supporting this hypothesis]
      Probability: [High / Medium / Low]

  Intervention Plan:
  ├── Action 1: [Specific change — e.g., "Test direct phone outreach for VP+ targets"]
  │   Owner: [Outreach Agent / Research Agent / etc.]
  │   Timeline: [Start date → evaluation date]
  │   Success Metric: [What improvement would validate this intervention]
  │
  ├── Action 2: [Specific change]
  │   Owner: [Agent]
  │   Timeline: [Dates]
  │   Success Metric: [Metric]
  │
  └── Action 3: [Specific change]
      Owner: [Agent]
      Timeline: [Dates]
      Success Metric: [Metric]

  Re-evaluation Date:        [Date — typically 30 days after intervention starts]
```

### Trend Tracking

Track conversion rates by stage over time to identify trends — not just point-in-time snapshots.

```
CONVERSION TREND — Last 6 Months

Stage              M-6     M-5     M-4     M-3     M-2     M-1     Trend
──────────────────────────────────────────────────────────────────────────
Discover→Qualify   38%     41%     39%     35%     32%     30%     ↓ Declining
Qualify→Engage     22%     24%     26%     25%     27%     28%     ↑ Improving
Engage→Propose     48%     50%     52%     51%     49%     47%     → Stable
Propose→Close      28%     30%     32%     35%     33%     31%     → Stable
──────────────────────────────────────────────────────────────────────────
```

A declining trend over 3+ months at any stage triggers a mandatory root cause investigation, even if the current rate is still within target range. The trend matters more than the snapshot.

---

## 5. Feedback to System Correction Matrix

This matrix defines exactly what changes when specific feedback signals are received. Every feedback signal has a defined owner and a defined response.

### Trigger and Targeting Corrections

| Feedback Signal | System Change | Owner | Priority |
|----------------|---------------|-------|----------|
| Trigger type X produces meetings at 2x average rate | Increase monitoring weight for Trigger X. Allocate more research agent time to sources that detect Trigger X. | Research Agent | High |
| Trigger type Y never converts past Qualify stage | Reduce scoring weight for Trigger Y by 50%. If still no conversion after 2 quarters, remove from active monitoring. | Research Agent | Medium |
| Leads from [specific industry segment] close at 3x rate | Increase ICP weight for that segment. Develop segment-specific outreach sequences. | Research Agent | High |
| Leads from [specific geography] consistently lose | Investigate whether geography is the issue (relationship gap, competitor stronghold, different market dynamics) or coincidence. | Research Agent | Medium |
| Small companies (<$500M revenue) never convert past Engage | Raise minimum company size threshold in ICP. Stop pursuing companies below threshold. | Research Agent | High |

### Outreach and Messaging Corrections

| Feedback Signal | System Change | Owner | Priority |
|----------------|---------------|-------|----------|
| Email angle A produces 3x the meeting rate of angle B | Rotate all cold sequences to angle A as primary. Test angle A variations. Retire angle B. | Outreach Agent | High |
| LinkedIn outperforms email for VP+ buyers | Shift channel allocation: LinkedIn primary for VP+ titles, email primary for Director and below. | Outreach Agent | High |
| Warm introductions close 3x faster than cold outreach | Invest in referral network expansion. Set goal: 30% of outreach sequences should be warm intros. | Outreach Agent | Critical |
| Direct mail produces highest meeting rate per dollar for C-suite | Develop direct mail program for top 50 target accounts. Budget $25-$50 per package. | Outreach Agent | Medium |
| Outreach timing: Tuesday-Thursday mornings produce 2x response rates | Schedule all first-touch emails for Tuesday-Thursday 7-9 AM in buyer's time zone. | Outreach Agent | Low |
| Buyers consistently cite a specific content piece as influential | Feature that content piece prominently in all sequences. Create follow-up content on the same theme. | Outreach Agent | Medium |

### Competitive and Positioning Corrections

| Feedback Signal | System Change | Owner | Priority |
|----------------|---------------|-------|----------|
| Competitor Z wins on price consistently | Develop counter-positioning: "Competitor Z charges less because they use junior staff. Our team averages 15 years of experience. The cost of re-work from junior teams exceeds the fee difference." | Competitive Agent | High |
| Competitor Z wins on relationships (Type F losses) | Map Competitor Z's relationship network. Identify where we can build parallel relationships. Target mutual contacts for warm introductions. | Competitive Agent + Outreach Agent | High |
| We lose when the buyer has never heard of us | Increase brand awareness activity. Publish more content. Speak at more conferences. Build LinkedIn presence. | All Agents | Medium |
| We win on speed (responding faster than competitors) | Codify speed as a competitive advantage. Set internal SLA: first outreach within 48 hours of trigger detection. | Research + Outreach Agents | Critical |
| We win on diagnostic approach (no competitor offers entry-level diagnostics) | Promote diagnostic approach aggressively as a differentiator. Develop diagnostic-specific marketing content. | All Agents | High |

### Revenue and Pricing Corrections

| Feedback Signal | System Change | Owner | Priority |
|----------------|---------------|-------|----------|
| Diagnostic type X converts to full engagement at 60%+ | Increase promotion of Diagnostic X. Consider developing Diagnostic X variations for sub-segments. | Revenue Agent | High |
| Diagnostic type Y converts below 30% | Investigate: Is the diagnostic poorly designed? Is the follow-on proposal too expensive? Are we presenting findings correctly? Fix the root cause or retire the diagnostic. | Revenue Agent | High |
| Average deal size declining over 3 months | Review package mix. Are we selling too many small engagements? Review pricing — are we discounting too aggressively? Review targeting — are we pursuing smaller companies? | Revenue Agent | High |
| Objection "too expensive" appearing in >30% of deal interactions | Develop stronger ROI proof points. Create value calculators for each package. Test pricing adjustments ($10K-$25K reductions) on a controlled set of proposals. | Revenue Agent + Outreach Agent | Medium |
| Referral leads convert at 3x cold leads | Double investment in referral generation. Target: 5 referral introductions per month. Develop a formal referral incentive program for satisfied clients and industry contacts. | All Agents | Critical |
| Conference leads close faster than any other channel | Increase conference budget. Target 4-6 conferences per year. Prioritize conferences where target buyers attend. | Revenue Agent | Medium |

---

## 6. Quarterly Performance Review Protocol

Every quarter, run a comprehensive system review. This is the revenue agent's most important recurring deliverable.

### Quarterly Intelligence Report Structure

```
QUARTERLY INTELLIGENCE REPORT — Q[N] [Year]

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

1. EXECUTIVE SUMMARY
   Revenue: $[X] closed (target: $[Y], [over/under] by $[Z])
   Pipeline: $[X] weighted (coverage ratio: [N]x)
   Win/Loss: [N] wins, [N] losses, [X]% win rate
   Key Insight: [One sentence — the single most important finding]

2. PIPELINE HEALTH
   ├── Coverage ratio:          [N]x (target: 3x-5x)
   ├── Stage velocity (avg):    [N] days (target: 120)
   ├── Stalled deals:           [N] (>50% over average time in stage)
   ├── New leads entered:       [N] (vs prior Q: [+/-N])
   └── Leads disqualified:      [N] (vs prior Q: [+/-N])

   Conversion Rates:
   ├── Discover → Qualify:      [X]% (target: 40%, trend: [↑/↓/→])
   ├── Qualify → Engage:        [X]% (target: 25%, trend: [↑/↓/→])
   ├── Engage → Propose:        [X]% (target: 50%, trend: [↑/↓/→])
   └── Propose → Close:         [X]% (target: 30%, trend: [↑/↓/→])

3. REVENUE PERFORMANCE
   ├── Q revenue (closed):      $[X]
   ├── Q forecast (start of Q): $[Y]
   ├── Forecast accuracy:       [X]%
   ├── YTD revenue:             $[X]
   ├── YTD target:              $[Y]
   ├── Annual forecast:         $[X]
   └── Annual target:           $[Y]

4. WIN/LOSS ANALYSIS
   Wins:
   ├── [Company A] — $[X] — Package [X] — Trigger: [X] — Cycle: [N] days
   ├── [Company B] — $[X] — Package [X] — Trigger: [X] — Cycle: [N] days
   └── Pattern: [What do the wins have in common?]

   Losses:
   ├── [Company C] — $[X] — Stage: [X] — Type: [A-F] — Reason: [X]
   ├── [Company D] — $[X] — Stage: [X] — Type: [A-F] — Reason: [X]
   └── Pattern: [What do the losses have in common?]

   Win/Loss Ratio:              [X]% (prior Q: [X]%, trend: [↑/↓/→])
   Average Win Value:           $[X] (prior Q: $[X])
   Average Win Cycle:           [N] days (prior Q: [N] days)

5. ATTRIBUTION ANALYSIS
   Top-Performing Triggers:     [Trigger A] ([N] deals, $[X] revenue)
                                [Trigger B] ([N] deals, $[X] revenue)

   Top-Performing Channels:     [Channel A] ([N] meetings, [X]% close rate)
                                [Channel B] ([N] meetings, [X]% close rate)

   Top-Performing Angles:       [Angle A] ([X]% meeting rate, [X]% close rate)
                                [Angle B] ([X]% meeting rate, [X]% close rate)

   Worst-Performing:            [Category] — [X] — Recommendation: [Fix / Retire / Monitor]

6. COMPETITIVE LANDSCAPE
   ├── New competitors observed:    [Names, positioning]
   ├── Competitor wins this Q:      [N] (companies: [X, Y, Z])
   ├── Competitor weaknesses found: [Specific gaps exploitable next Q]
   └── Positioning adjustments:     [Recommended changes]

7. MARKET SIGNALS
   ├── Demand indicators:       [Expanding / Stable / Contracting]
   ├── Key regulatory changes:  [FERC orders, state PUC decisions, etc.]
   ├── M&A activity:            [Deals in market, announced, closed]
   ├── Capital program signals: [Rate case filings, IRP updates, grid mod plans]
   └── Hiring signals:          [Industry hiring trends that indicate project activity]

8. SYSTEM IMPROVEMENT RECOMMENDATIONS
   ├── Recommendation 1: [What to change, why, expected impact]
   ├── Recommendation 2: [What to change, why, expected impact]
   ├── Recommendation 3: [What to change, why, expected impact]
   └── Recommendation 4: [What to change, why, expected impact]

9. NEXT QUARTER PRIORITIES
   ├── Revenue target:          $[X]
   ├── Pipeline target:         $[X] weighted ([N]x coverage)
   ├── Focus triggers:          [Top 3 trigger types to prioritize]
   ├── Focus channels:          [Top 2 channels to invest in]
   ├── Focus accounts:          [Top 10 target accounts]
   └── System changes:          [Specific agent adjustments]

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Filed to: campaigns/consultant-lead-agent/quarterly-reviews/Q[N]-[Year]-intelligence-report.md
```

---

## 7. Annual System Calibration

Once per year — in January — the entire system undergoes a full recalibration. This is the deepest review in the system. It touches every agent, every assumption, every metric.

### Annual Calibration Checklist

**1. Revenue Model Rebuild**
- Pull all closed deal data from the trailing 12 months
- Recalculate blended average deal size from actual data (replace estimates with actuals)
- Recalculate conversion rates at every stage from actual data
- Recalculate average deal cycle from actual data
- Set new annual revenue target with owner
- Derive all pipeline requirements from the new model
- Update revenue math model in revenue-agent.md

**2. Trigger Taxonomy Review**
- List all trigger types used in the trailing 12 months
- Rank by: leads generated, meetings produced, deals closed, revenue produced
- Add new trigger categories discovered during the year (new regulatory frameworks, new market entrants, new technology disruptions)
- Retire trigger categories that produced zero closed revenue in 12 months
- Update trigger scoring weights based on actual conversion data
- Update research-agent.md with revised trigger taxonomy

**3. Channel Effectiveness Recalibration**
- Calculate revenue per dollar spent for each outreach channel
- Calculate meeting rate, proposal rate, and close rate for each channel
- Identify channels with declining effectiveness (market saturation, spam filter changes, platform algorithm changes)
- Identify emerging channels worth testing (new social platforms, podcast advertising, industry association partnerships)
- Update outreach-agent.md with revised channel allocation

**4. Pricing Model Update**
- Aggregate all pricing signals from 12 months of deal interactions
- Calculate actual blended rate (total revenue / total hours delivered)
- Compare to competitive pricing intelligence
- Evaluate price-to-win correlation
- Set new pricing floors and ceilings for each package
- Update diagnostic pricing based on conversion data
- Update proposal templates with new pricing

**5. Package Portfolio Review**
- Win rate by package (which packages win most often?)
- Average deal size by package (which packages generate the most revenue per deal?)
- Client satisfaction by package (which packages produce the happiest clients?)
- Referral rate by package (which packages generate the most referrals?)
- Consider adding new packages to address unmet demand discovered during the year
- Consider modifying packages that have low win rates or low satisfaction
- Consider retiring packages that no one buys
- Update packaged-offers.md with any changes

**6. Competitive Landscape Overhaul**
- Full refresh of competitor profiles
- New entrants identified during the year
- Exits or mergers among competitors
- Competitor positioning shifts (new services, new markets, new pricing)
- Counter-positioning playbook update
- Update competitive-intel-agent.md

**7. Market Expansion Evaluation**
- Should we enter new geographic markets? (New RTO regions, new states)
- Should we enter new industry segments? (Midstream, renewable developers, industrial)
- Should we develop new service lines? (Technology implementation, managed services, training)
- Revenue potential vs investment required for each expansion option
- Decision: Expand, hold, or contract

**8. Brand Memory Update**
- Review all entries in learnings.md
- Archive patterns that are no longer relevant
- Consolidate similar patterns
- Highlight the 10 most important lessons from the year
- Update positioning language based on what actually resonates with buyers

**9. Goal Setting**
- Set annual revenue target
- Derive pipeline requirements
- Set quarterly milestones
- Set activity targets (leads, meetings, proposals per month)
- Assign ownership and accountability
- Document in annual-plan.md

### Annual Calibration Output

The annual calibration produces a single document: Annual System Calibration Report

This report contains:
1. Prior year performance summary (revenue, pipeline, win/loss, attribution)
2. Updated revenue math model with new assumptions
3. Updated conversion rates and stage weights
4. Updated pricing model
5. Updated trigger taxonomy and channel allocation
6. Package portfolio changes
7. Competitive landscape update
8. Market expansion decisions
9. Goals and targets for the new year
10. Agent-by-agent change log (what each agent should do differently)

Filed to: campaigns/consultant-lead-agent/annual-reviews/[Year]-system-calibration.md

This document is the operating manual for the system for the next 12 months. Every agent reads it and adjusts its behavior accordingly. It is the single source of truth for how the system operates.
