---
name: proposal-from-diagnostic-template
parent: consultant-lead-agent
version: 1.0
purpose: "Convert diagnostic findings into structured consulting proposals"
---

# Proposal From Diagnostic Template

## 1. Purpose

When a diagnostic is complete, the proposal should write itself. The diagnostic findings ARE the proposal's foundation. You do not need to "figure out what to propose" — the diagnostic already told you what is broken, how badly, and what it costs the client. The proposal simply maps those findings to a phased remediation plan with clear pricing and expected outcomes.

This template eliminates the gap between "diagnostic delivered" and "proposal sent." That gap is where deals die. The client loved the diagnostic. They saw the findings. They nodded along. Then you said "I'll put together a proposal" and it took two weeks. By then, their urgency faded, their budget got redirected, or a competitor walked in the door. The proposal must be delivered within 5 business days of the diagnostic presentation — ideally within 3.

This template does three things:
1. Defines the exact structure of every consulting proposal
2. Maps diagnostic findings directly to proposal sections so the conversion is mechanical, not creative
3. Provides pricing guidelines so you can price with confidence instead of guessing

---

## 2. Proposal Structure

Every proposal follows this 8-section structure. No section is optional. Total length is 8-12 pages. Executives do not read 50-page proposals — they skim for findings, approach, price, and timeline. Make those sections impossible to miss.

---

### Section 1: Executive Summary (1 page)

This is the only page some decision-makers will read. It must stand on its own.

**Opening sentence:** Reference the diagnostic by name and date.
"Following the [Diagnostic Name] conducted [Date Range], this proposal outlines a phased approach to address the [number] critical findings identified during our assessment."

**Three key findings (ranked by business impact, not severity):**
Each finding should be one sentence with a quantified impact.

Format:
1. [Finding]: [Business impact in dollars, time, or risk]
2. [Finding]: [Business impact in dollars, time, or risk]
3. [Finding]: [Business impact in dollars, time, or risk]

Example:
1. Interconnection queue backlog of 847 applications is 3.2x industry benchmark, creating an estimated $12M in deferred revenue annually.
2. Data quality errors in 23% of submitted applications are driving a 40% rework rate, consuming approximately 2,400 staff-hours per year.
3. Absence of real-time performance dashboards means SLA compliance is unknown until quarterly audit, by which point corrective action is too late.

**Recommended approach (one sentence):**
"We recommend a three-phase engagement beginning with [Phase 1 focus], progressing to [Phase 2 focus], and establishing [Phase 3 focus] for sustained performance."

**Investment range:**
"Total investment for Phases 1-3 is $[X] to $[Y] over [timeframe], with Phase 1 beginning within [weeks] of engagement approval."

**Expected outcomes (quantified):**
"Based on diagnostic findings and comparable engagements, we project [specific outcome 1], [specific outcome 2], and [specific outcome 3] within [timeframe]."

---

### Section 2: Situation Assessment (1-2 pages)

Demonstrate that you understand their world before proposing solutions.

**Context paragraph:**
Why was the diagnostic conducted? What business conditions prompted the assessment? Reference the trigger event that created the engagement (M&A, regulatory change, performance decline, new leadership mandate).

Example: "[Company] is navigating a significant increase in interconnection applications driven by FERC Order 2023 compliance requirements and accelerating renewable energy development in the [state] market. The [Diagnostic Name] was commissioned to assess current operational capacity, identify bottlenecks, and recommend improvements to meet projected demand."

**Scope paragraph:**
What was examined during the diagnostic? Be specific about boundaries — what was in scope and what was explicitly out of scope.

Example: "The diagnostic examined the end-to-end interconnection application lifecycle from initial submission through final approval, including intake processes, engineering review workflows, data management systems, staffing models, and SLA compliance tracking. Distribution-level interconnections below 10 MW were out of scope for this assessment."

**Methodology paragraph:**
How were findings developed? List the activities performed during the diagnostic.

Example: "Findings were developed through interviews with 14 staff members across 4 departments, analysis of 6 months of application processing data (January-June 2026), review of current standard operating procedures, benchmarking against 3 peer utilities, and direct observation of the application intake and engineering review processes."

**Key stakeholders paragraph:**
Who was involved in the diagnostic? Name titles (not individuals, for the proposal document) who participated.

Example: "The diagnostic was supported by the VP of Grid Operations, Director of Interconnection Services, Manager of Engineering Review, and representatives from IT, Legal, and Regulatory Affairs."

**Current state summary:**
One paragraph synthesizing the overall health of the function assessed. Not the individual findings — those come next — but the big picture.

Example: "The interconnection services function is currently operating under significant strain. Application volumes have increased 280% over 36 months while staffing has grown only 15%. Current processes were designed for a lower volume environment and have not been updated to accommodate the surge. The result is a growing backlog, declining SLA compliance, increasing customer complaints, and staff burnout that threatens retention of experienced engineers."

---

### Section 3: Key Findings (2-3 pages)

Present each finding in a consistent format. Each finding should make the reader feel the pain and understand the cost of inaction.

**Finding #[N]: [Title]**

Description: What is the problem? State it plainly in 2-3 sentences. Avoid jargon where possible.

Evidence: What data supports this finding? Cite specific numbers from the diagnostic.

Business Impact: What does this cost the company in dollars, time, risk, or competitive position? Quantify whenever possible.

Root Cause: Why does this problem exist? (Not blame — systemic reasons.)

Risk If Unaddressed: What happens if the company does nothing about this finding?

---

**Example Finding #1: Interconnection Queue Backlog Exceeds Industry Benchmarks by 3.2x**

Description: The current interconnection queue contains 847 active applications, with an average processing time of 14.7 months. Industry benchmarks for comparable utilities indicate an average processing time of 4.6 months. The backlog has grown by 35% in the past 12 months with no indication of slowing.

Evidence:
- 847 active applications in queue (source: application tracking spreadsheet, June 2026)
- Average processing time: 14.7 months vs. 4.6-month industry benchmark (source: peer utility benchmarking study)
- 35% backlog growth in 12 months (source: year-over-year comparison)
- 12 applications older than 24 months still unresolved

Business Impact:
- Estimated $12M in deferred revenue from delayed interconnection agreements
- 47 customer complaints filed with the PUC in the past 6 months (up from 8 the prior year)
- Regulatory risk: PUC has indicated potential penalties for continued SLA non-compliance
- Reputational damage affecting future developer relationships

Root Cause: Process was designed for 200 applications/year volume. Current volume is 600+/year. No triage system exists — all applications are processed first-come-first-served regardless of complexity, size, or impact. Engineering review bottleneck caused by single-reviewer workflow with no parallel processing capability.

Risk If Unaddressed: Continued backlog growth will trigger PUC enforcement action, estimated penalty exposure of $2-5M annually. Developer frustration will drive projects to competing service territories. Staff attrition will accelerate as burnout intensifies.

---

**Example Finding #2: Data Quality Errors Driving 40% Rework Rate**

Description: 23% of submitted interconnection applications contain data quality errors that require correction before engineering review can begin. These errors trigger a rework cycle that adds an average of 6.2 weeks to processing time and consumes approximately 2,400 staff-hours per year in back-and-forth communication with applicants.

Evidence:
- 23% error rate in submitted applications (source: sample audit of 200 recent applications)
- Average rework cycle: 6.2 weeks from error identification to corrected resubmission
- 2,400 staff-hours/year consumed by rework (source: time tracking analysis)
- Most common errors: incorrect single-line diagrams (34%), missing site control documentation (28%), incomplete technical specifications (22%)

Business Impact:
- 2,400 staff-hours/year = approximately $180K in direct labor cost spent on rework
- 6.2-week rework cycle compounds the backlog — each error adds 1.5 months to an already-long process
- Staff frustration with repetitive error correction contributes to turnover risk

Root Cause: Application submission portal does not validate required fields or document formats before submission. No standardized checklist is provided to applicants. Error correction is handled via email without a structured workflow, leading to multiple rounds of incomplete corrections.

Risk If Unaddressed: As application volumes continue to increase, rework volume will grow proportionally. Without validation at intake, the problem compounds indefinitely.

---

**Additional Findings (Summarized)**

For findings beyond the top 3, provide a summary table:

| # | Finding | Severity | Business Impact | Recommended Phase |
|---|---------|:--------:|-----------------|:-----------------:|
| 4 | [Title] | High/Med/Low | [Impact summary] | Phase [1/2/3] |
| 5 | [Title] | High/Med/Low | [Impact summary] | Phase [1/2/3] |
| 6 | [Title] | High/Med/Low | [Impact summary] | Phase [1/2/3] |

**Risk Assessment: Cost of Inaction**

Summarize what happens if the client does nothing. This is not a scare tactic — it is a factual projection based on diagnostic data.

"Based on current trajectory, if no corrective action is taken within the next 12 months, [Company] can expect:
- Queue backlog to grow to [X] applications (from current [Y])
- PUC penalty exposure of $[X] annually
- Staff attrition of [X]% based on current burnout indicators
- Estimated total cost of inaction: $[X] over [timeframe]"

---

### Section 4: Recommended Approach (2-3 pages)

Map findings directly to a phased remediation plan. Each phase should address specific findings and produce specific deliverables.

**Phase 1: Quick Wins (30 days)**

Objective: Address the highest-impact findings that can be resolved quickly. Build momentum and demonstrate value before tackling systemic changes.

| Deliverable | Addresses Finding # | Timeline | Team Required |
|-------------|:-------------------:|:--------:|---------------|
| [Specific deliverable 1] | [#] | Week 1-2 | [Roles needed] |
| [Specific deliverable 2] | [#] | Week 2-3 | [Roles needed] |
| [Specific deliverable 3] | [#] | Week 3-4 | [Roles needed] |

Expected Outcomes:
- [Quantified outcome 1 — e.g., "Reduce rework rate from 23% to under 10% through intake validation"]
- [Quantified outcome 2]
- [Quantified outcome 3]

Investment: $[X]

Example Phase 1 deliverables for the interconnection queue diagnostic:
- Application intake validation checklist and portal field requirements (eliminates 80% of common submission errors)
- Queue triage protocol (categorize applications by complexity, route to appropriate review track)
- Backlog priority matrix (identify the 50 oldest applications and create expedited resolution plan)
- Interim performance dashboard (weekly queue metrics visible to leadership)

**Phase 2: System Improvements (60-90 days)**

Objective: Address root causes identified in the diagnostic. Implement process redesigns, technology solutions, and organizational changes that create lasting improvement.

| Deliverable | Addresses Finding # | Timeline | Team Required |
|-------------|:-------------------:|:--------:|---------------|
| [Specific deliverable 1] | [#] | Month 2 | [Roles needed] |
| [Specific deliverable 2] | [#] | Month 2-3 | [Roles needed] |
| [Specific deliverable 3] | [#] | Month 3 | [Roles needed] |

Expected Outcomes:
- [Quantified outcome 1 — e.g., "Reduce average processing time from 14.7 months to under 6 months"]
- [Quantified outcome 2]
- [Quantified outcome 3]

Investment: $[X]

Example Phase 2 deliverables for the interconnection queue diagnostic:
- Parallel engineering review workflow (multiple reviewers processing simultaneously, reducing bottleneck)
- Automated SLA tracking and compliance reporting system
- Staffing model redesign with hiring recommendations and interim augmentation plan
- Standard operating procedures updated for current volume levels
- Applicant self-service portal for status tracking (reduces inquiry calls by 60%)

**Phase 3: Sustained Performance (Ongoing)**

Objective: Maintain improvements, monitor performance, and provide ongoing advisory support to prevent regression.

| Deliverable | Addresses Finding # | Timeline | Team Required |
|-------------|:-------------------:|:--------:|---------------|
| [Specific deliverable 1] | [All] | Monthly | [Roles needed] |
| [Specific deliverable 2] | [All] | Quarterly | [Roles needed] |

Expected Outcomes:
- [Quantified outcome 1 — e.g., "Sustained SLA compliance above 90%"]
- [Quantified outcome 2]

Investment: $[X]/month

Example Phase 3 deliverables:
- Monthly performance review and trend analysis
- Quarterly process optimization recommendations
- On-call advisory support for complex application scenarios
- Annual benchmarking update against peer utilities

---

### Section 5: Team and Qualifications (1 page)

**Lead Consultant:**
Name, title, and 3-4 sentences of directly relevant experience. Focus on experience that matches this engagement, not a full biography.

Example: "[Name] brings 15 years of experience in utility operations and energy infrastructure management, including direct experience managing interconnection queue operations for [peer utility]. [He/She] has led [X] similar engagements resulting in an average queue processing time reduction of [X%] across [X] utilities."

**Supporting Team Members:**
For each supporting team member, provide name, role on this engagement, and one sentence of relevant qualification.

**Relevant Case Studies:**

Case Study 1: [Title — anonymized if needed]
- Client: [Description — "Midwest investor-owned utility" if anonymized]
- Challenge: [One sentence]
- Approach: [One sentence]
- Result: [Quantified — "Reduced queue backlog by 60% in 6 months, saving $3.2M annually"]

Case Study 2: [Same format]

Case Study 3: [Same format]

**Client References:**
"References from comparable engagements are available upon request."
(Do not include reference contact information in the proposal. Provide separately when requested.)

---

### Section 6: Investment and Timeline (1 page)

**Fee Structure:**

| Phase | Scope Summary | Duration | Fixed Fee |
|:-----:|---------------|:--------:|:---------:|
| Phase 1 | Quick Wins | 30 days | $[X] |
| Phase 2 | System Improvements | 60-90 days | $[X] |
| Phase 3 | Sustained Performance | Ongoing | $[X]/month |
| | **Total (Phases 1+2)** | **3-4 months** | **$[X]** |

All fees are fixed fee per phase, not hourly. This protects the client from scope creep risk and aligns our incentives — we are motivated to deliver results efficiently, not to extend timelines.

**Payment Schedule:**

| Milestone | Payment Amount | Trigger |
|-----------|:--------------:|---------|
| Engagement kickoff | [X]% of Phase 1 fee | Signed agreement + kickoff meeting |
| Phase 1 delivery | [X]% of Phase 1 fee | Phase 1 deliverables accepted |
| Phase 2 kickoff | [X]% of Phase 2 fee | Phase 2 start |
| Phase 2 midpoint | [X]% of Phase 2 fee | Midpoint review and approval |
| Phase 2 delivery | [X]% of Phase 2 fee | Phase 2 deliverables accepted |
| Phase 3 | Monthly | Monthly invoice, net 30 |

**Timeline:**

```
Week 1-2:    Phase 1 — Intake validation + triage protocol
Week 3-4:    Phase 1 — Backlog priority matrix + interim dashboard
Week 5-8:    Phase 2 — Parallel workflow design + SLA system
Week 9-12:   Phase 2 — Staffing model + SOP updates + portal
Week 13+:    Phase 3 — Monthly reviews + quarterly optimization
```

**Assumptions:**
- Client provides timely access to data, systems, and stakeholders as outlined in the kickoff meeting
- Key stakeholder availability for weekly check-ins (30 minutes)
- Decisions on recommendations made within [X] business days of presentation
- Scope is limited to the function assessed in the diagnostic; adjacent functions are out of scope

**Exclusions:**
- Software licensing or procurement costs (if applicable)
- Hardware or infrastructure purchases
- Third-party integration costs
- Legal review of contracts or regulatory filings
- Changes in scope requested after engagement start (handled via change order)

**Change Order Process:**
If either party identifies a need to adjust scope, timeline, or deliverables, a written change order will be submitted for mutual approval before any additional work begins. Change orders will include revised scope, timeline impact, and fee adjustment (if any).

---

### Section 7: Success Metrics (half page)

Define how both parties will know the engagement succeeded.

| Metric | Baseline (from Diagnostic) | Target (Post-Engagement) | Measurement Method |
|--------|:--------------------------:|:------------------------:|-------------------|
| [Metric 1 — e.g., Queue processing time] | [Current value] | [Target value] | [How measured] |
| [Metric 2 — e.g., Rework rate] | [Current value] | [Target value] | [How measured] |
| [Metric 3 — e.g., SLA compliance] | [Current value] | [Target value] | [How measured] |
| [Metric 4 — e.g., Staff hours on rework] | [Current value] | [Target value] | [How measured] |

**Reporting cadence:** [Weekly status report during Phases 1-2; monthly performance report during Phase 3]

---

### Section 8: Terms and Next Steps (half page)

**Proposal validity:** This proposal is valid for 30 calendar days from the date of delivery. After 30 days, scope and pricing may be subject to revision.

**How to accept:** Sign the attached engagement letter and return via email to [contact]. A countersigned copy will be returned within 2 business days.

**Start date availability:** We can begin Phase 1 within [X] business days of signed engagement letter.

**Kick-off meeting agenda:**
1. Confirm diagnostic findings and priorities
2. Align on Phase 1 deliverables and success criteria
3. Identify key contacts and data access requirements
4. Establish communication cadence and reporting format
5. Set Phase 1 milestone dates

**Questions or discussion:** Contact [Name] at [email] or [phone] to discuss any aspect of this proposal.

---

## 3. Diagnostic-to-Proposal Mapping Matrix

For each diagnostic type, the findings map to specific proposal phases and deliverables. This matrix is the automation layer — when the diagnostic outputs data, the proposal sections populate mechanically.

### Utility Interconnection Queue Diagnostic

| Diagnostic Finding | Threshold | Proposal Phase | Proposal Deliverable |
|-------------------|-----------|:--------------:|---------------------|
| Queue backlog > 2x industry benchmark | >2x = critical | Phase 1 | Intake triage redesign with complexity-based routing |
| Data quality errors > 10% of submissions | >10% = action needed | Phase 1 | Application validation checklist and portal field requirements |
| Average processing time > 2x benchmark | >2x = critical | Phase 2 | Parallel engineering review workflow implementation |
| SLA compliance < 80% | <80% = action needed | Phase 2 | Automated SLA tracking and compliance reporting system |
| Staffing gap > 2 FTEs below calculated need | >2 FTE gap = hire | Phase 2 | Staffing model redesign with augmented team and training plan |
| No real-time performance dashboard | Absent = build | Phase 2 | Real-time interconnection performance dashboard |
| Customer complaint volume > 10/quarter | >10 = urgent | Phase 1 | Applicant communication protocol and status portal |
| No documented SOPs for current volume | Absent = build | Phase 2 | Updated standard operating procedures for current workload |
| Staff overtime > 15% of regular hours | >15% = burnout risk | Phase 1 | Workload redistribution and immediate relief plan |
| No backlog reduction plan | Absent = build | Phase 1 | 90-day backlog reduction plan with priority matrix |

### Post-Acquisition Integration Sprint

| Diagnostic Finding | Threshold | Proposal Phase | Proposal Deliverable |
|-------------------|-----------|:--------------:|---------------------|
| Day 1 readiness gaps identified | Any gaps = act | Phase 1 | Day 1 execution plan with responsibility matrix |
| Cultural integration risk: High | High = act now | Phase 1 | Integration communication plan and town hall schedule |
| IT system overlap > 3 redundant platforms | >3 = rationalize | Phase 2 | IT integration PMO and system rationalization roadmap |
| Duplicate vendor contracts identified | Any dupes = act | Phase 2 | Vendor rationalization with consolidated contract plan |
| Key talent retention risk for 5+ roles | >5 at-risk = urgent | Phase 1 | Retention program with stay bonuses and career path mapping |
| No integration governance structure | Absent = build | Phase 1 | Integration steering committee charter and cadence |
| Regulatory approval gaps | Any gaps = urgent | Phase 1 | Regulatory compliance checklist and filing timeline |
| Synergy targets undefined or unrealistic | Undefined = define | Phase 2 | Synergy identification and tracking framework |
| Combined organization chart not designed | Absent = design | Phase 2 | Target operating model and org design |
| Customer communication plan absent | Absent = build | Phase 1 | Customer communication plan and FAQ document |

### Turnaround Readiness Assessment

| Diagnostic Finding | Threshold | Proposal Phase | Proposal Deliverable |
|-------------------|-----------|:--------------:|---------------------|
| Work scope clarity score < 6/10 | <6 = rework scope | Phase 1 | Scope definition sprint with work package breakdown |
| Contractor mobilization lead time > 8 weeks | >8 wk = too slow | Phase 1 | Pre-mobilization program with vendor pre-qualification |
| Safety incident rate > industry average | Above avg = urgent | Phase 1 | Safety management overhaul and training program |
| Schedule risk score > 7/10 | >7 = critical | Phase 2 | Critical path scheduling with resource-loaded logic |
| Cost overrun history > 15% on past 3 projects | >15% = systemic | Phase 2 | Project controls implementation with earned value tracking |
| No lessons-learned process from prior turnarounds | Absent = build | Phase 2 | Structured lessons-learned program and knowledge base |
| Material procurement lead time not tracked | Not tracked = risk | Phase 1 | Long-lead material identification and expediting plan |
| QA/QC program inadequate for scope | Inadequate = fix | Phase 2 | QA/QC program redesign with inspection and test plans |
| No integrated schedule (separate contractor schedules) | Fragmented = integrate | Phase 2 | Integrated master schedule with contractor alignment |
| Contingency budget < 10% of total | <10% = underfunded | Phase 1 | Risk-adjusted contingency analysis and budget recommendation |

### Regulatory Compliance Program Assessment

| Diagnostic Finding | Threshold | Proposal Phase | Proposal Deliverable |
|-------------------|-----------|:--------------:|---------------------|
| Compliance gaps in 3+ regulatory areas | >3 areas = systemic | Phase 1 | Gap closure priority matrix with timeline |
| No compliance tracking system | Absent = build | Phase 2 | Compliance management system implementation |
| Staff training below required hours | Below req = urgent | Phase 1 | Training program design and scheduling |
| Audit findings unresolved > 90 days | >90 days = escalate | Phase 1 | Audit finding resolution plan with accountability |
| No self-assessment program | Absent = build | Phase 2 | Internal audit and self-assessment program |
| Regulatory filing deadlines missed in past year | Any missed = urgent | Phase 1 | Filing calendar and automated reminder system |

---

## 4. Pricing Guidelines

### Phase Pricing Ratios

Use the diagnostic fee as the pricing anchor for subsequent phases:

| Phase | Ratio to Diagnostic Fee | Rationale |
|:-----:|:-----------------------:|-----------|
| Phase 1 (Quick Wins) | 0.5x - 1.5x | Limited duration, high-impact fixes. Should feel like a "no-brainer" investment relative to the value delivered. |
| Phase 2 (System Improvements) | 2x - 4x | Largest phase. Most deliverables. Systemic changes require more time and expertise. |
| Phase 3 (Ongoing) | 15-25% of Phase 2 fee/month | Maintenance and optimization. Recurring revenue that scales with client needs. |

**Example pricing build:**

If the diagnostic fee was $25K:
- Phase 1: $15K - $37.5K (pick $25K — matches diagnostic, feels proportional)
- Phase 2: $50K - $100K (pick $75K — significant but justified by scope)
- Phase 3: $7.5K - $18.75K/month (pick $12K/month — 6-month minimum)

Total engagement value: $25K (diagnostic) + $25K (Phase 1) + $75K (Phase 2) + $72K (Phase 3, 6 months) = $197K total from a single $25K diagnostic entry point.

### Value-Based Pricing Justification

Every proposal should include a return-on-investment calculation:

```
Annual cost of the problem (from diagnostic findings):     $[X]
Total engagement investment (Phases 1-3):                  $[Y]
Net benefit in Year 1:                                     $[X - Y]
ROI:                                                       [X/Y]x return

Example:
  Annual cost of interconnection queue backlog:              $12M in deferred revenue
  Annual cost of rework labor:                               $180K
  Annual cost of PUC penalty exposure:                       $2M (midpoint estimate)
  TOTAL ANNUAL COST OF INACTION:                             $14.18M

  Total engagement investment:                               $197K
  Net benefit in Year 1:                                     $13.98M
  ROI:                                                       72x return
```

Even if the client discounts these projections by 90%, the ROI is still 7.2x. The engagement pays for itself many times over.

### Risk-Based Pricing Justification

When value-based ROI is hard to quantify, use risk-based framing:

"The cost of this engagement represents [X%] of the estimated penalty exposure if compliance issues remain unresolved. It is effectively an insurance policy with a guaranteed deliverable — you get the process improvements regardless of whether the penalty materializes."

### Competitive Pricing Calibration

Know what the buyer's alternatives cost:

| Competitor Type | Typical Pricing | Your Position |
|----------------|----------------|---------------|
| Big 4 / large firms (Burns & McDonnell, B&V) | $300-500/hour, $200K+ for comparable scope | Price at 60-75% of their rate. Deliver faster. More senior attention. |
| Mid-size firms (regional engineering firms) | $175-300/hour, $100K-200K for comparable scope | Price at parity or slight premium. Differentiate on specialization and speed. |
| Independent consultants | $150-250/hour, $50K-100K for comparable scope | Price at premium to independents. Differentiate on structured methodology and deliverables. |
| Internal team (do it themselves) | Staff time + opportunity cost, often underestimated | Show that internal teams lack bandwidth and specialization. Your fixed fee is cheaper than pulling staff off revenue-generating work. |

---

## 5. Proposal Delivery Protocol

### Preparation (Before Delivery)

- Print two copies of the proposal (one for the client, one for your reference during presentation)
- Prepare a 15-20 minute walkthrough of the key sections (do not read the proposal aloud — present the highlights)
- Create a one-page leave-behind summary for the decision committee (many proposals get forwarded to people who were not in the meeting)
- Anticipate 3-5 objections and prepare responses (too expensive, timeline too long, can we do this internally, what about competitor X)
- Confirm attendees for the proposal presentation meeting (you want the economic buyer in the room, not just the technical champion)

### Delivery Format

- **In person or video call, NEVER just email.** If you email a proposal without presenting it, you lose control of the narrative. The buyer reads it alone, fixates on the price page, and compares it to alternatives without your context.
- **Format:** Clean PDF. Professional header with your logo. Consistent formatting. No clip art, stock photos, or filler content. The proposal should look like a serious business document, not a marketing brochure.
- **Length:** 8-12 pages maximum. If you cannot make the case in 12 pages, you are not being clear enough.

### Presentation Flow (20-30 minutes)

1. **Opening (2 minutes):** Thank them for the diagnostic engagement. Reference what you learned and what impressed you about their team.
2. **Findings recap (5 minutes):** Summarize the top 3 findings with quantified impact. Do not re-present the full diagnostic — they already saw it. This is a reminder of why they need to act.
3. **Recommended approach (10 minutes):** Walk through each phase. Explain what will be delivered, by when, and what outcomes to expect. Connect each deliverable back to a specific finding.
4. **Investment (3 minutes):** Present the pricing with confidence. Reference the ROI calculation. Pause for questions.
5. **Team and qualifications (2 minutes):** Briefly introduce who will do the work. Reference case studies that are directly relevant.
6. **Next steps (3 minutes):** Explain how to accept. Offer a start date. Suggest the kickoff meeting agenda.
7. **Questions (5+ minutes):** Open the floor. Answer directly. If you don't know, say so and commit to a follow-up.

### Post-Delivery Follow-Up

- **Within 24 hours:** Send a thank-you email. Attach the proposal PDF and the one-page summary. Reference one specific point from the meeting discussion.
- **Within 48 hours:** Send any follow-up information promised during the meeting (additional case studies, reference contacts, clarifying details).
- **Day 7:** Check in if you haven't heard back. Ask if they have questions, not "have you made a decision yet."
- **Day 14:** Follow up with a relevant article or trigger event that reinforces the urgency of the findings.
- **Day 21:** Direct ask: "What would be helpful for moving this forward? Is there additional information the decision team needs?"
- **Day 30:** Proposal expiration reminder. "This proposal is valid through [date]. Happy to discuss any updates needed if timing has shifted."

### Leave-Behind Summary (One Page)

This single page gets forwarded to the CFO, the board, the procurement team — people who will never read the full proposal. It must contain:

```
[Your Company Logo]

ENGAGEMENT SUMMARY: [Client Name] — [Engagement Title]

SITUATION: [2 sentences — what was assessed and why]

KEY FINDINGS:
1. [Finding + impact]
2. [Finding + impact]
3. [Finding + impact]

RECOMMENDED APPROACH:
Phase 1 (30 days):  [One sentence + fee]
Phase 2 (60-90 days): [One sentence + fee]
Phase 3 (Ongoing):  [One sentence + fee/month]

TOTAL INVESTMENT: $[X] — EXPECTED ROI: [X]x in Year 1

NEXT STEP: [Name] is available to begin Phase 1 within [X] weeks.
Contact: [Email] | [Phone]
```

That is the entire document. One page. Everything a decision-maker needs to say "yes" or "tell me more."
